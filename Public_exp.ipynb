{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "os.environ[\"OPENAI_API_KEY\"] = \"Your OpenAI API Key\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "\n",
    "# ChatGPT Function Define\n",
    "user_struct = lambda x: {\"role\": \"user\", \"content\": x}\n",
    "system_struct = lambda x: {\"role\": \"system\", \"content\": x}\n",
    "assistant_struct = lambda x: {\"role\": \"assistant\", \"content\": x}\n",
    "\n",
    "servant = \"Output the answer without any additional information.\"\n",
    "\n",
    "def gpt_answer(messages, model = \"gpt-3.5-turbo\"):\n",
    "    response = client.chat.completions.create(\n",
    "        model = model,\n",
    "        messages = messages,\n",
    "        temperature = 0.2,\n",
    "        timeout = 20\n",
    "    )\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def extract_option(response):\n",
    "    match = re.match(r'^[A-D]', response.strip())\n",
    "    if match:\n",
    "        return match.group(0)\n",
    "    return response\n",
    "\n",
    "def extract_type(response):\n",
    "    match = re.match(r'^(Conceptual|Calculation)', response.strip(), re.IGNORECASE)\n",
    "    if match:\n",
    "        return match.group(0)\n",
    "    return response\n",
    "\n",
    "def extract_numbers_from_string(input_string):\n",
    "    numbers = re.findall(r'\\d+', input_string)\n",
    "\n",
    "    if numbers:\n",
    "        return int(numbers[0])\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import hub\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.agents import AgentExecutor, create_react_agent\n",
    "from langchain_experimental.tools import PythonREPLTool\n",
    "\n",
    "def get_answer_langchain(question_and_choices, model_name = \"gpt-3.5-turbo\"):\n",
    "    llm = ChatOpenAI(\n",
    "        model_name = model_name,\n",
    "        temperature = 0,\n",
    "        timeout = 20\n",
    "    )\n",
    "\n",
    "    tools = [PythonREPLTool()]\n",
    "    # PythonREPLTool() may refer to a Python interactive environment (REPL) typically used for interactively executing and testing Python code.\n",
    "    # This function or object can provide an interactive interface that allows users to input and execute Python code, and immediately see the execution results.\n",
    "\n",
    "    instructions = \"\"\"You are an agent designed to answer accounting multiple choice questions.\n",
    "    You have access to a python REPL, which you can use to execute python code.\n",
    "    If you get an error, debug your code and try again.\n",
    "    Only use the output of your code to answer the question.\n",
    "    You might know the answer without running any code, but you should still run the code to get the answer.\n",
    "    If it does not seem like you can write code to answer the question, just return \"I don't know\" as the answer.\n",
    "    \"\"\"\n",
    "\n",
    "    base_prompt = hub.pull(\"langchain-ai/react-agent-template\")\n",
    "    prompt = base_prompt.partial(instructions=instructions)\n",
    "\n",
    "    agent = create_react_agent(llm, tools, prompt)\n",
    "    agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True, handle_parsing_errors=True)\n",
    "    response = agent_executor.invoke({\"input\": question_and_choices})\n",
    "    \n",
    "    try:\n",
    "        return(response[\"output\"])\n",
    "    except KeyError:\n",
    "        return(\"Error in message chat completions.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "def get_answer_RAG(question_and_choices, vectordb, model_name = \"gpt-3.5-turbo\"):\n",
    "    llm = ChatOpenAI(\n",
    "        model_name = model_name,\n",
    "        temperature = 0,\n",
    "        timeout = 20\n",
    "    )\n",
    "\n",
    "    retriever = vectordb.as_retriever()\n",
    "    \n",
    "    qa = RetrievalQA.from_chain_type(\n",
    "        llm = llm, \n",
    "        chain_type = \"stuff\", \n",
    "        retriever = retriever, \n",
    "        verbose = True\n",
    "    )\n",
    "    response = qa.invoke({\"query\": question_and_choices})\n",
    "    \n",
    "    try:\n",
    "        return response[\"result\"]\n",
    "    except KeyError:\n",
    "        return \"Error in message chat completions.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_answer_RAR(question_and_choices, vectordb, model_name = \"gpt-3.5-turbo\"):\n",
    "    llm = ChatOpenAI(\n",
    "        model_name = model_name,\n",
    "        temperature = 0,\n",
    "        timeout = 20\n",
    "    )\n",
    "\n",
    "    retriever = vectordb.as_retriever()\n",
    "\n",
    "    qa = RetrievalQA.from_chain_type(\n",
    "        llm = llm,\n",
    "        chain_type = \"stuff\",\n",
    "        retriever = retriever,\n",
    "        verbose = True\n",
    "    )\n",
    "    \n",
    "    rag_response = qa.invoke({\"query\": question_and_choices})\n",
    "    \n",
    "    try:\n",
    "        additional_info = rag_response[\"result\"]\n",
    "    except KeyError:\n",
    "        return \"Error in message chat completions (RAG step).\"\n",
    "\n",
    "    tools = [PythonREPLTool()]\n",
    "    instructions = \"\"\"You are an agent designed to answer accounting calculation multiple choice questions.\n",
    "    You have access to a python REPL, which you can use to execute python code.\n",
    "    If you get an error, debug your code and try again.\n",
    "    Only use the output of your code to answer the question.\n",
    "    You might know the answer without running any code, but you should still run the code to get the answer.\n",
    "    If it does not seem like you can write code to answer the question, just return \"I don't know\" as the answer.\n",
    "    \"\"\"\n",
    "    \n",
    "    base_prompt = hub.pull(\"langchain-ai/react-agent-template\")\n",
    "    prompt = base_prompt.partial(instructions=instructions)\n",
    "\n",
    "    agent = create_react_agent(llm, tools, prompt)\n",
    "    agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True, handle_parsing_errors=True)\n",
    "    \n",
    "    # Combine the additional info with the original question and choices\n",
    "    print(additional_info)\n",
    "    combined_input = f\"{question_and_choices}\\n\\nAdditional Information:\\n{additional_info}\"\n",
    "    response = agent_executor.invoke({\"input\": combined_input})\n",
    "    \n",
    "    try:\n",
    "        return response[\"output\"]\n",
    "    except KeyError:\n",
    "        return \"Error in message chat completions (Python REPL step).\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################################\n",
    "### These steps only need to run once for building vector database ###\n",
    "######################################################################\n",
    "\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.document_loaders import PyMuPDFLoader\n",
    "\n",
    "# the following are loading textbooks' PDF\n",
    "loader1 = PyMuPDFLoader(\"Text book 1\")\n",
    "documents1 = loader1.load()\n",
    "loader2 = PyMuPDFLoader(\"Text book 2\")\n",
    "documents2 = loader2.load()\n",
    "loader3 = PyMuPDFLoader(\"Text book 3\")\n",
    "documents3 = loader3.load()\n",
    "loader4 = PyMuPDFLoader(\"Text book 4\")\n",
    "documents4 = loader4.load()\n",
    "# ......\n",
    "\n",
    "documents = documents1 + documents2 + documents3 + documents4 \n",
    "\n",
    "# split document into small chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 200,   # max size of each chunk\n",
    "    chunk_overlap = 10   # overlap to ensure context continuity\n",
    ")\n",
    "chunks = text_splitter.split_documents(documents)\n",
    "\n",
    "# Create embeddings\n",
    "embedding = OpenAIEmbeddings() # use OpenAi's embedding model\n",
    "\n",
    "# Persist embeddings to disk\n",
    "persist_directory = 'book_db'\n",
    "vectordb = Chroma.from_documents(documents = chunks, embedding = embedding, persist_directory = persist_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################################################\n",
    "### Once build the vector database, you only need to run these code for next experiments ###\n",
    "############################################################################################\n",
    "\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "persist_directory = 'book_db'\n",
    "embedding = OpenAIEmbeddings()\n",
    "vectordb = Chroma(persist_directory = persist_directory, embedding_function = embedding) # load stored vector database "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "file_path = \"./CPA_example.json\" # data path for questions\n",
    "with open(file_path, 'r', encoding='utf-8') as file:\n",
    "    data = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "question_type = {\"AUD\": [0, 0], \"BEC\": [0, 0], \"FAR\": [0, 0], \"REG\": [0, 0]}\n",
    "correct_answer = 0\n",
    "\n",
    "for index, item in enumerate(data[\"questions\"]):\n",
    "    \n",
    "    id = item[\"question_id\"]\n",
    "    question = item['question']\n",
    "    choices = [item['choice_a'], item['choice_b'], item['choice_c'], item['choice_d']]\n",
    "    answer = item['answer']\n",
    "    \n",
    "    # Task router: determine question type \n",
    "    type_prompt = \"I have a CPA exam question, and I need to know whether it is a calculation question or a conceptual question. Here is the question:\\n\"\n",
    "    type_prompt += f\"{question}\\n A. {choices[0]}\\n B. {choices[1]}\\n C. {choices[2]}\\n D. {choices[3]}\"\n",
    "    type_prompt += \"\\nCan you please determine if this is a calculation question or a conceptual question?\"\n",
    "    messages = [\n",
    "        system_struct(servant),\n",
    "        user_struct(type_prompt)\n",
    "    ]\n",
    "    type_response = gpt_answer(messages).strip()\n",
    "    Type = extract_type(type_response)\n",
    "    # print(f\"{Type}\")\n",
    "    \n",
    "    if \"Conceptual\" in Type or \"conceptual\" in Type:\n",
    "        prompt = \"This is the conceptual question of CPA exam, please think that:\\n\"\n",
    "        prompt += f\"{question}\\n A. {choices[0]}\\n B. {choices[1]}\\n C. {choices[2]}\\n D. {choices[3]}\"\n",
    "        prompt += \"\\nWhat is the correct option? Output the answer without any additional information.\"\n",
    "        LLM_response = get_answer_RAG(prompt, vectordb, model_name=\"gpt-4o-mini\").strip() # you can change default openAI model for others\n",
    "    else: # Calculation\n",
    "        prompt = \"Let's think step by step:\\n\"\n",
    "        prompt += f\"{question}\\n A. {choices[0]}\\n B. {choices[1]}\\n C. {choices[2]}\\n D. {choices[3]}\"\n",
    "        prompt += \"\\nThe correct answer for previous options is A, B, C, or D?\\n\"\n",
    "        LLM_response = get_answer_RAR(prompt, vectordb, model_name=\"gpt-4o-mini\").strip() # you can change default openAI model for others\n",
    "            \n",
    "    extracted_option = extract_option(LLM_response)\n",
    "    print(f\"{id} answer: {extracted_option}, correct answer: {answer}\")\n",
    "    \n",
    "    if(extracted_option == answer):\n",
    "        print(\"Correct!!\")\n",
    "        correct_answer += 1\n",
    "        question_type[item[\"type\"]][0] += 1\n",
    "    else:\n",
    "        print(\"Wrong!!\")\n",
    "    \n",
    "    question_type[item[\"type\"]][1] += 1\n",
    "    # print(question_type)\n",
    "            \n",
    "    time.sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(question_type[\"AUD\"])\n",
    "print(f\"AUD: {question_type['AUD'][0] / question_type['AUD'][1]}\")\n",
    "\n",
    "print(question_type[\"BEC\"])\n",
    "print(f\"BEC: {question_type['BEC'][0] / question_type['BEC'][1]}\")\n",
    "\n",
    "print(question_type[\"FAR\"])\n",
    "print(f\"FAR: {question_type['FAR'][0] / question_type['FAR'][1]}\")\n",
    "\n",
    "print(question_type[\"REG\"])\n",
    "print(f\"REG: {question_type['REG'][0] / question_type['REG'][1]}\")\n",
    "\n",
    "total_question  = question_type[\"AUD\"][1] + question_type[\"BEC\"][1] + question_type[\"FAR\"][1] + question_type[\"REG\"][1]\n",
    "print(f\"\\nTotal accuracy: {correct_answer / total_question}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llamaGPU",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
